{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVhQ+qFqobsajDZhCgCTYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aurioldegbelo/sis2025/blob/main/2025_SIS_Demo_2_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topics: Data Modelling and Search Models\n",
        "* Expansion via multi-query generation\n",
        "* Task decomposition\n"
      ],
      "metadata": {
        "id": "f-f_Xm04pxpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "qIYgAUmRqYae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4313b8be-0749-471d-9d3b-8ed0f10e0309"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "9EqIHkDgqa8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e655047-dfa8-41eb-aa7e-8f177fa491e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/467.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m460.8/467.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expansion (Multi-Query)"
      ],
      "metadata": {
        "id": "FizpL4nAqGmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukt5Sk1IpkdV",
        "outputId": "edcf2009-22e1-4769-a7c1-c894d9003989"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. Can you tell me the capital city of Germany?',\n",
              " '2. Which city serves as the capital of Germany?',\n",
              " '3. What is the main city that functions as the capital of Germany?',\n",
              " '4. Do you know the capital of Germany?',\n",
              " '5. Could you provide information on the capital city of Germany?']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Adapted from https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Multi Query: Different Perspectives\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | llm\n",
        "    | output_parser\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "generate_queries.invoke({\"question\": \"What is the capital of Germany?\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another example of template/prompt for multi-query generation\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | llm\n",
        "    | output_parser\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "generate_queries.invoke({\"question\": \"What is the geometry of Hessen?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBXYEEDcr5Fc",
        "outputId": "5b4395ad-b7a1-4948-f434-43e7e16d0496"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. What are the geographical features of Hessen?',\n",
              " '2. How is the landscape of Hessen shaped?',\n",
              " '3. What are the boundaries of Hessen?',\n",
              " '4. What is the topography of Hessen like?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decomposition"
      ],
      "metadata": {
        "id": "0pqo50czqIAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "prompt_for_decomposition = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "decompose_problem = prompt_for_decomposition | llm | output_parser | (lambda x: x.split(\"\\n\"))\n",
        "\n",
        "question1 = \"What is the capital of Germany?\"\n",
        "question2 = \"Which cities are north of Germany?\"\n",
        "question3 = \"What is the geometry of Germany?\"\n",
        "question4 = \"What is the population of Germany?\"\n",
        "question5 = \"What are hotels near the central train station in Münster, Germany?\"\n",
        "\n",
        "#decompose_problem.invoke({\"question\": question1})\n",
        "decompose_problem.invoke({\"question\": question2})\n",
        "#decompose_problem.invoke({\"question\": question5})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZMYO-zntsmj",
        "outputId": "01846f73-4140-4fba-d478-23395c48e5cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. What are the northernmost cities in Europe?',\n",
              " '2. Which cities in Scandinavia are located north of Germany?',\n",
              " '3. Are there any major cities in the Baltic states that are north of Germany?']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}