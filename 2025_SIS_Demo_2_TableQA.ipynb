{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE/QUhS8zdKTII5OZQHRil",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aurioldegbelo/sis2025/blob/main/2025_SIS_Demo_2_TableQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topics: Data Modelling and Search Models\n",
        "* Question anwering over tables"
      ],
      "metadata": {
        "id": "dVETDLv8zhgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the data from the url"
      ],
      "metadata": {
        "id": "XBL2BzQ-UcPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Get table data\n",
        "def get_table_data(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # store all tables in the tables list\n",
        "    tables = []\n",
        "    geometry_data = response.json()\n",
        "    # loop through the dataset and convert tabular data to pandas dataframes\n",
        "    for doc in geometry_data:\n",
        "        table = pd.DataFrame(doc[\"data\"], columns=doc[\"header\"])\n",
        "        tables.append(table)\n",
        "\n",
        "    return tables, geometry_data\n",
        "\n",
        "tables, geometry_data = get_table_data(\"https://raw.githubusercontent.com/aurioldegbelo/sis2025/refs/heads/main/vector_data/data_table.json\")\n",
        "print(len(tables))\n",
        "tables[0]"
      ],
      "metadata": {
        "id": "9k2GGJbDV1gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tables[1]"
      ],
      "metadata": {
        "id": "WfE5bdvoWl85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the table to include information about title and description"
      ],
      "metadata": {
        "id": "HJ0O5W4yaEGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# include the content of the table and its title + description\n",
        "def preprocess_tables_with_metadata(tables: list, geometry_data):\n",
        "    processed = []\n",
        "    # loop through all tables\n",
        "    for index, table in enumerate(tables):\n",
        "        # convert the table to csv and\n",
        "        processed_table = \"\\n\".join([geometry_data[index]['title'], geometry_data[index]['description'], table.to_csv(index=False)])\n",
        "        # add the processed table to processed list\n",
        "        processed.append(processed_table)\n",
        "    return processed\n",
        "\n",
        "processed_table = preprocess_tables_with_metadata(tables, geometry_data)\n",
        "print(processed_table[0])"
      ],
      "metadata": {
        "id": "5y5B618dZcke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_table[1])"
      ],
      "metadata": {
        "id": "9sA5VpgxZgIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a retriever and load a model for question answering"
      ],
      "metadata": {
        "id": "7rSP4EIwaMUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pinecone sentence_transformers"
      ],
      "metadata": {
        "id": "ZZXMEyXrW2F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"PINECONE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")"
      ],
      "metadata": {
        "id": "P844mwIqViA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize retriever\n",
        "def initialize_retriever():\n",
        "    import torch\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    # set device to GPU if available\n",
        "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # load the table embedding model from huggingface models hub\n",
        "    retriever = SentenceTransformer(\"deepset/all-mpnet-base-v2-table\")\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def create_pinecone_index():\n",
        "    # initialize pinecone\n",
        "    import os\n",
        "    from pinecone import Pinecone, PodSpec\n",
        "\n",
        "    pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "\n",
        "    pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "    # create the pinecone index (see also https://docs.pinecone.io/guides/getting-started/quickstart)\n",
        "    index_name = \"table-qa-pc\"\n",
        "\n",
        "    # check if the table-qa index exists\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        # create the index if it does not exist\n",
        "        pc.create_index(\n",
        "            index_name,\n",
        "            dimension=768,\n",
        "            metric=\"cosine\",\n",
        "            spec=PodSpec(environment=\"gcp-starter\")\n",
        "            )\n",
        "\n",
        "    # connect to table-qa index we created\n",
        "    index = pc.Index(index_name)\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "def insert_embeddings_into_index (processed_tables, retriever, index):\n",
        "\n",
        "    from tqdm.auto import tqdm\n",
        "    # we will use batches of 64\n",
        "    batch_size = 64\n",
        "\n",
        "    for i in tqdm(range(0, len(processed_tables), batch_size)):\n",
        "        # find end of batch\n",
        "        i_end = min(i+batch_size, len(processed_tables))\n",
        "        # extract batch\n",
        "        batch = processed_tables[i:i_end]\n",
        "        # generate embeddings for batch\n",
        "        emb = retriever.encode(batch).tolist()\n",
        "        # create unique IDs ranging from zero to the total number of tables in the dataset\n",
        "        ids = [f\"{idx}\" for idx in range(i, i_end)]\n",
        "        # add all to upsert list\n",
        "        to_upsert = list(zip(ids, emb))\n",
        "        # upsert/insert these records to pinecone\n",
        "        index.upsert(vectors=to_upsert)\n",
        "\n",
        "    return index\n",
        "\n",
        "def load_tapas_model():\n",
        "    # https://huggingface.co/google/tapas-base-finetuned-wtq\n",
        "    from transformers import pipeline, TapasTokenizer, TapasForQuestionAnswering\n",
        "    model_name = \"google/tapas-base-finetuned-wtq\"\n",
        "    # load the tokenizer and the model from huggingface model hub\n",
        "    tokenizer = TapasTokenizer.from_pretrained(model_name)\n",
        "    model = TapasForQuestionAnswering.from_pretrained(model_name, local_files_only=False)\n",
        "    # load the model and tokenizer into a question-answering pipeline\n",
        "    pipe = pipeline(task =\"table-question-answering\",  model=model, tokenizer = tokenizer)\n",
        "    return pipe\n",
        "\n",
        "def load_tapex_model():\n",
        "    # https://huggingface.co/microsoft/tapex-base?library=transformers\n",
        "    from transformers import pipeline\n",
        "    pipe = pipeline(task = \"table-question-answering\", model=\"microsoft/tapex-base\")\n",
        "    return pipe\n",
        "\n",
        "\n",
        "def get_relevant_table(user_query, retriever, index, tables):\n",
        "    # generate embedding for the query\n",
        "    xq = retriever.encode([user_query]).tolist()\n",
        "    # query pinecone index to find the table containing answer to the query\n",
        "    result = index.query(vector=xq, top_k=1)\n",
        "    # return the relevant table from the tables list\n",
        "    print(result)\n",
        "    return tables[int(result[\"matches\"][0][\"id\"])]\n",
        "\n",
        "def get_answer_from_table(table, user_query, pipe):\n",
        "    # run the table and query through the question-answering pipeline\n",
        "    # astype(str) because the the tapas tokenizer expects the data of the table to be text only, see https://huggingface.co/docs/transformers/model_doc/tapas\n",
        "    answers = pipe(table=table.astype(str), query=user_query)\n",
        "    return answers\n",
        "\n",
        "# main function to get an answer from a table\n",
        "def query_tables (pipe, user_query, retriever, final_index, tables):\n",
        "    relevant_table = get_relevant_table(user_query, retriever, final_index, tables)\n",
        "    #print(relevant_table)\n",
        "    answer = get_answer_from_table(relevant_table, user_query, pipe)\n",
        "    #print(answer)\n",
        "    return answer, relevant_table"
      ],
      "metadata": {
        "id": "Rxg1KCHKUJHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model separately, to avoid loading it every time we have a new question\n",
        "tapas_pipe = load_tapas_model() # TAPAS model\n",
        "#tapex_pipe = load_tapex_model() # TAPEX model"
      ],
      "metadata": {
        "id": "t_Vo6uC_caxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play around with questions and answers"
      ],
      "metadata": {
        "id": "MIi-e3gChjFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the vector store\n",
        "data_url = \"https://raw.githubusercontent.com/aurioldegbelo/sis2025/refs/heads/main/vector_data/data_table.json\"\n",
        "\n",
        "tables, geometry_data = get_table_data(data_url)\n",
        "retriever = initialize_retriever() # pick a model to compute the embedding\n",
        "processed_tables = preprocess_tables_with_metadata(tables, geometry_data)\n",
        "initial_index = create_pinecone_index()\n",
        "final_index  = insert_embeddings_into_index(processed_tables, retriever, initial_index)\n",
        "final_index.describe_index_stats()"
      ],
      "metadata": {
        "id": "ylVDArYsGope"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"How many states in the database?\"\n",
        "question2 = \"How many geometries in the the database?\"\n",
        "question3 = \"What is the population of Hessen?\"\n",
        "question4 = \"What is the area of Hessen?\"\n",
        "question5 = \"What is the capital of Hessen?\"\n",
        "question6 = \"What is the geometry of Hessen?\"\n",
        "question7 = \"What are the geometries of Hessen and Niedersachsen?\"\n",
        "question8 = \"What is the url of the geometry of Hessen?\"\n",
        "\n",
        "answer, relevant_table = query_tables (tapas_pipe, question4, retriever, final_index, tables)\n",
        "\n",
        "print(answer)\n",
        "#print(relevant_table)"
      ],
      "metadata": {
        "id": "MZ9hzdoiU0XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Links\n",
        "* [Tapas Tutorial](https://rocm.blogs.amd.com/artificial-intelligence/TaPas/README.html)\n",
        "* [Resources on LLMS for Tabular Data](https://github.com/SpursGoZmy/Awesome-Tabular-LLMs)"
      ],
      "metadata": {
        "id": "VPijGSUuhT5q"
      }
    }
  ]
}